{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merge & Cleaning\n",
    "In this notebook, I will merge all data and check what's missing. \n",
    "I'll also use this section to figure out what kind of preprocessing will be necessary and how to extract text information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard1 = pd.read_pickle('PKL/raw_data_Harvard_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard2 = pd.read_pickle('PKL/raw_data_Harvard_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd1 = pd.read_pickle('PKL/raw_data_RISD_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd2 = pd.read_pickle('PKL/raw_data_RISD_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma = pd.read_csv('DATA/MoMA_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvard Museum\n",
    "---\n",
    "Let's look at the Harvard data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard = pd.concat([harvard1, harvard2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "harvard = harvard.drop_duplicates(subset = ['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing image\n",
    "Drop if it's missing image url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard = harvard.dropna(subset = ['primaryimageurl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Consolidation\n",
    "I'll consolidate scattered info into ...\n",
    "1. period\n",
    "2. culture\n",
    "3. medium\n",
    "4. title\n",
    "5. description\n",
    "6. palette\n",
    "7. date\n",
    "8. name of the artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnesting Subcategories\n",
    "There are sub-categories, which I'll unnest first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images, worktypes, colors, people are dictionary nested in the list. We'll need to extract information from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(x, name):\n",
    "    ''' \n",
    "    INPUT: a list of dictionary, key name\n",
    "    OUTPUT: key value, consolidated as one string if many\n",
    "    '''\n",
    "    if isinstance(x, list):\n",
    "        if len(x) == 1:\n",
    "            return x[0][name]\n",
    "        else:\n",
    "            inst = []\n",
    "            for i in range(len(x)):\n",
    "                val = x[i][name]\n",
    "                if val not in inst and val != None:\n",
    "                    inst.append(val)\n",
    "            if len(inst) > 1:\n",
    "                return ', '.join(inst)\n",
    "            elif len(inst) == 1:\n",
    "                return inst[0]\n",
    "            else:\n",
    "                return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image info\n",
    "imageinfo = ['description', 'alttext', 'publiccaption']\n",
    "for item in imageinfo: \n",
    "    harvard[f'img_{item}'] = harvard['images'].apply(lambda x: extract_info(x, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worktype\n",
    "harvard['worktype'] = harvard['worktypes'].apply(lambda x: extract_info(x, 'worktype'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color\n",
    "harvard['color'] = harvard['colors'].apply(lambda x: extract_info(x, 'hue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for artist, do the same but only take the artist roles\n",
    "def extract_info_artist(x, name):\n",
    "    if isinstance(x, list):\n",
    "        if len(x) == 1:\n",
    "            return x[0][name]\n",
    "        else:\n",
    "            inst = []\n",
    "            for i in range(len(x)):\n",
    "                if x[i]['role'] == 'Artist':\n",
    "                    val = x[i][name]\n",
    "                    if val not in inst and val != None:\n",
    "                        inst.append(val)\n",
    "            if len(inst) > 1:\n",
    "                return ', '.join(inst)\n",
    "            elif len(inst) == 1:\n",
    "                return inst[0]\n",
    "            else:\n",
    "                return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard['artist'] = harvard['people'].apply(lambda x: extract_info(x, 'displayname'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard = harvard.drop(['images', 'worktypes', 'colors', 'people'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining columns\n",
    "I'll combine columns in this manner.\n",
    "1. period: if datebegin is 0 use century\n",
    "2. description: combine everything: 'style', 'commentary', 'description', 'labeltext', 'img_description', 'img_alttext', 'img_publiccaption'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard['century'] = np.where(harvard['century'].isnull(), harvard['dated'], harvard['century'])\n",
    "harvard['period'] = np.where(harvard['period'].isnull(), harvard['century'], harvard['period'])\n",
    "harvard['period'] = np.where(harvard['datebegin'] == 0, harvard['period'], harvard['datebegin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list = ['style', 'commentary', 'description', 'labeltext', 'img_description', 'img_alttext', 'img_publiccaption']\n",
    "for c in desc_list:\n",
    "    harvard[c] = harvard[c].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard['all_description'] = harvard[['style', 'commentary', 'description', 'labeltext', \n",
    "         'img_description', 'img_alttext', 'img_publiccaption']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard = harvard[['id', 'period', 'medium', 'title', 'culture', 'color', 'artist', 'all_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard['source'] = 'harvard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOMA\n",
    "---\n",
    "Now similar steps for MOMA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing image\n",
    "drop missing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma = moma.dropna(subset = ['ThumbnailURL'])\n",
    "moma = moma[moma.Classification == 'Painting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['source'] = 'moma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col_list = ['ObjectID', 'Title', 'Artist', 'Nationality', 'Date', 'Medium', 'source']\n",
    "moma = moma[col_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISD\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd = pd.concat([risd1, risd2], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing images\n",
    "Remove if the image is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd = risd[[len(x) > 0 for x in risd['images']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location\n",
    "Get the primary maker's location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nationality(x):\n",
    "    '''\n",
    "    given a dictionaries nested in a list, \n",
    "    return the first person's nationality\n",
    "    '''\n",
    "    if x:\n",
    "        ind = list(x)[0]['nationality']\n",
    "        if ind: \n",
    "            return ind[0]\n",
    "        else: \n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd['nationality'] = risd.makers.apply(lambda x: get_nationality(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating info\n",
    "1. only paintings or other art-like work on paper \n",
    "2. culture: culture if empty, place, if empty nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting column\n",
    "col_list = ['id', 'culture', 'dating', 'description', 'mediumTechnique', 'place', 'primaryMaker',  \n",
    "            'title', 'type', 'nationality']\n",
    "risd = risd[col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only paintings\n",
    "art_list = ['Paintings']\n",
    "risd = risd[[any(item in x[0] for item in art_list) for x in risd['type']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "RISD datasets don't have missing values. They just have empty values. Let's change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd = risd.replace('', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# culture\n",
    "risd['place'] = np.where(risd['place'].isnull(), risd['nationality'], risd['place'])\n",
    "risd['culture'] = np.where(risd['culture'].isnull(), risd['place'], risd['culture'])\n",
    "risd = risd.drop(['place', 'nationality'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd = risd.drop(['type'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd['source'] = 'risd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purging\n",
    "Now check if there is an actual matching image file, if not drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_img_list = os.listdir('IMAGES/HARVARD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(x, list_):\n",
    "    '''\n",
    "    return the filename if the file exists in list_\n",
    "    otherwise np.nan\n",
    "    '''\n",
    "    fn = f'{x}.jpg' \n",
    "    if fn in list_:\n",
    "        return fn\n",
    "    else: return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard['image'] = harvard['id'].map(lambda x: check_image(x, harvard_img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma_img_list = os.listdir('IMAGES/MOMA')\n",
    "moma['image'] = moma['ObjectID'].map(lambda x: check_image(x, moma_img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image_risd(x, list_):\n",
    "    '''\n",
    "    return the filename if the file exists in list_\n",
    "    otherwise np.nan\n",
    "    '''\n",
    "    fn = f'risd_{x}.jpg' \n",
    "    if fn in list_:\n",
    "        return fn\n",
    "    else: return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd_img_list = os.listdir('IMAGES/RISD')\n",
    "risd['image'] = risd['id'].map(lambda x: check_image_risd(x, risd_img_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging\n",
    "now let's merge all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first making consistent column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard = harvard.rename(columns={'all_description': 'description'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma = moma.rename(columns = {'ObjectID': 'id', 'Title': 'title', 'Artist': 'artist', 'Nationality': 'culture', \n",
    "                             'Date': 'period', 'Medium': 'medium'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "risd = risd.rename(columns = {'dating': 'period', 'mediumTechnique': 'medium', 'primaryMaker': 'artist'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([harvard, risd, moma], ignore_index = True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df[~full_df.image.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "full_df = full_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_pickle('PKL/merged_artworks_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset\n",
    "I'm going to filter the dataset to include only recent art where abstraction started to be incorporated in art. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-digits\n",
    "tmp = [re.sub('th', '00', str(x)) for x in full_df.period]\n",
    "\n",
    "# for later, period names without numbers\n",
    "# list of [x for x in tmp if re.match('^\\D*$', x)]\n",
    "\n",
    "# remove all non-digits\n",
    "tmp = [re.sub('[^\\d-]', '', str(x)) for x in tmp]\n",
    "# remove all number after '-'\n",
    "tmp = [re.sub('-.*', '', str(x)) for x in tmp]\n",
    "\n",
    "# turn them into numbers\n",
    "tmp = [int(x) if x else 0 for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [int(str(x)[0:4]) if x > 2030 else x for x in tmp ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upto 1800, combine all century, after that, go by decades\n",
    "cond = [(tmp < 1300) | (tmp > 2030), \n",
    "       tmp < 1900]\n",
    "val = [0, (tmp//100)*100]\n",
    "full_df['period'] = np.select(cond, val, (tmp//10)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df[full_df['period'] >= 1900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_pickle('PKL/merged_artworks_recent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captions\n",
    "---\n",
    "Now we need data with captions. The goal is to get a series of sentences that describe an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mediums, I'll do following cleaning.\n",
    "1. remove all texts in parentheses \n",
    "2. remove all texts following 'reading' (refers to specific signature)\n",
    "3. if words are too long (e.g. 30), cut last words\n",
    "4. change semi-colons to 'and'\n",
    "5. remove \\r\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_medium_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # remove texts in parentheses\n",
    "        text = re.sub('\\(.*\\)', '', text)\n",
    "        # remove after 'reading'\n",
    "        text = re.sub('reading.*', '', text)\n",
    "        # change semi-colons\n",
    "        text = re.sub(';', ' and ', text)\n",
    "        # remove alphabet starts with \\\n",
    "        text = re.sub('\\[a-z]{1}', '', text)\n",
    "        # if longer than 30, remove the last part \n",
    "        if len(text.split()) > 30:\n",
    "            text = ' '.join(text.split()[0:30])\n",
    "            \n",
    "        return ' '.join(text.split()) # clean up whitespae\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['medium'] = full_df['medium'].apply(lambda x: clean_medium_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title\n",
    "For titles, I'll run these cleaning steps. One of the biggest problem is that the title includes a lot of proper nouns that can meaninglessly increase the dimensions. So based on our data, I'll try to create a dicitonary that filters out the proper nouns and change to appropriate pronoun.\n",
    "\n",
    "1. if the text format is person's name + years, don't include. e.g. texts(optional text 4 digits-4 digits), change it to a portrait of a person\n",
    "2. remove texts in parentheses\n",
    "3. change colons to 'about'\n",
    "4. remove texts after 'replica'\n",
    "5. removed texts after 'Identified'\n",
    "6. remove digits and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation + string.digits\n",
    "table_ = str.maketrans('', '', punctuations)\n",
    "\n",
    "def clean_title_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub('.*\\(\\d{4}\\-\\d{4}\\)', 'portrait of a person', text)\n",
    "        text = re.sub('\\(.*\\)', '', text)\n",
    "        text = re.sub(':', ' about ', text)\n",
    "        text = re.sub('replica.*', '', text)\n",
    "        text = re.sub('Identified.*', '', text)\n",
    "        # remove alphabet starts with \\\n",
    "        text = re.sub('\\[a-z]{1}', '', text)\n",
    "        text = str.translate(text, table_)\n",
    "        # if longer than 30, remove the last part \n",
    "        if len(text.split()) > 30:\n",
    "            text = ' '.join(text.split()[0:30])\n",
    "            \n",
    "        return ' '.join(text.split()) # clean up whitespae\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = full_df['title'].apply(lambda x: clean_title_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bit too many unique words, especially because art title does not have any standard. This could be problematic as it can lead to too big of dimensions. I'll try to reduce the dimension using similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the corpus \n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word that only appears once, find the closest word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [x.lower() for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def consolidate_words(textlist, thresh = .8):\n",
    "    '''\n",
    "    Takes a list of texts to run iteration\n",
    "    Change word that occurs only once to similar word in the text\n",
    "    Return consolidated list \n",
    "    '''\n",
    "    list_of_text = textlist.copy()\n",
    "    wordcounts = Counter(' '.join(list_of_text).split())    \n",
    "    words_once = [k for k, v in wordcounts.items() if v == 1]\n",
    "    other_words = [k for k, v in wordcounts.items() if v > 1]\n",
    "    \n",
    "    tokens = nlp(' '.join(other_words))\n",
    "    replacement_dict = {}\n",
    "    for word in words_once:\n",
    "\n",
    "        word_token = nlp(word)\n",
    "        max_similarity = thresh\n",
    "\n",
    "        for tk in tokens:\n",
    "            # find the maximum similarity above threshold\n",
    "            sim_score = word_token.text, tk.text, word_token.similarity(tk)\n",
    "            if 1 > sim_score[2] > max_similarity:\n",
    "                replacement_dict[word] = sim_score[1]\n",
    "                max_similarity = sim_score[2]\n",
    "        try:\n",
    "            print(word, 'to', replacement_dict[word])\n",
    "        except KeyError:\n",
    "            print('no matching word for', word)\n",
    "        \n",
    "    \n",
    "    for i, text in enumerate(list_of_text):\n",
    "        \n",
    "        text = text.split()\n",
    "        for j, te in enumerate(text):\n",
    "            if te in replacement_dict: \n",
    "                text[j] = replacement_dict[te]\n",
    "        list_of_text[i] = ' '.join(text)\n",
    "            \n",
    "    return list_of_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_tmp = consolidate_words(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3239"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(' '.join(tmp).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(' '.join(new_tmp).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['caption'] = new_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It reduced about 100 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting\n",
    "Now I'll save the files with more consistent and appropriate img_id. Then save a img_id and the description (medium and title combined) as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate img ids\n",
    "full_df['img_id'] = full_df['source'] + '_' + full_df['id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = full_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_caption1 = dict(zip(select_df.img_id, select_df.caption))\n",
    "img_caption2 = dict(zip(select_df.img_id, select_df.medium))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risd set\n",
    "fnames = select_df[select_df['source'] == 'risd']['img_id']\n",
    "for fn in fnames:\n",
    "    shutil.move(f'IMAGES/RISD/{fn}.jpg', f'IMAGES/paintings/{fn}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvard set\n",
    "sel = select_df[select_df['source'] == 'harvard'].reset_index()\n",
    "old_fnames = sel['id']\n",
    "new_fnames = sel['img_id']\n",
    "for i in range(len(old_fnames)):\n",
    "    shutil.move(f'IMAGES/HARVARD/{old_fnames[i]}.jpg', \n",
    "                f'IMAGES/paintings/{new_fnames[i]}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moma set\n",
    "sel = select_df[select_df['source'] == 'moma'].reset_index()\n",
    "old_fnames = sel['id']\n",
    "new_fnames = sel['img_id']\n",
    "for i in range(len(old_fnames)):\n",
    "    shutil.move(f'IMAGES/MOMA/{old_fnames[i]}.jpg', \n",
    "                f'IMAGES/paintings/{new_fnames[i]}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PKL/img_captions1.pkl', 'wb') as fp:\n",
    "    pickle.dump(img_caption1, fp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('PKL/img_captions2.pkl', 'wb') as fp:\n",
    "    pickle.dump(img_caption2, fp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Cleaning\n",
    "---\n",
    "Below is not directly related to this project, but it's here in case any further analysis is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium\n",
    "This would be multi-label case. \n",
    "I'll first consolidate all the unique mediums, categorize them and then turn them into a list of binary columns for each categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# special chracters\n",
    "special_chars = string.punctuation + string.digits\n",
    "\n",
    "# stopwords\n",
    "sw = stopwords.words('english')\n",
    "sw += ['color', 'mounted', 'synthetic', 'hanging', 'painted', 'signature', 'reading', 'two', 'light', 'artist', \n",
    "      'one', 'opaque', 'colors', 'folding', 'three', 'one', 'frame', 'painting', 'parts', 'album', 'seal', \n",
    "       'nan', 'style', 'seals']\n",
    "\n",
    "def text_preprocess(x):\n",
    "    # remove punctuations and digits\n",
    "    if isinstance(x, str):\n",
    "        table_ = str.maketrans('', '', special_chars)\n",
    "        text = str.translate(x, table_)\n",
    "        text = text.split()\n",
    "        clean_text = [wnl.lemmatize(w.lower()) for w in text]\n",
    "        return [w for w in clean_text if w not in sw]\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['medium'] = full_df['medium'].apply(lambda x: text_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get frequencies\n",
    "from collections import Counter\n",
    "#top_30_medium = \n",
    "top_30_medium = list(dict(Counter(np.sum([x for x in full_df['medium'] if isinstance(x, list)])).most_common(30)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for med in top_30_medium: \n",
    "    full_df[med] = [str(med) in x if isinstance(x, list) else 'h' for x in full_df['medium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['other_medium'] = np.where(np.sum(full_df.iloc[:, -30:], axis = 1) == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
