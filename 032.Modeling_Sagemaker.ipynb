{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Sagemaker\n",
    "This notebook is the Iteration 5 from model 1 set up for the purpose of running on AWS Sagemaker.  \n",
    "It takes the pickles of extracted features, dictionary of descriptions, and train, test, val lists from Model 1.  \n",
    "*To-do: For the full deployment, feature extraction portion should migrate into the AWS.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import tensorflow as tf\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import pickle\n",
    "import numpy as np\n",
    "from s3fs.core import S3FileSystem\n",
    "\n",
    "import time\n",
    "\n",
    "from SCRIPT.sequence_generator import *\n",
    "from SCRIPT.evaluation_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-descriptor'\n",
    "pkl_dir = f's3://{bucket}/PKL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files into sagemaker\n",
    "This step is not necessary for Tensorflow. Its only purpose is to get other parameters from our data for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(fname):\n",
    "    s3_file = S3FileSystem()\n",
    "    return pickle.load(s3_file.open('{}/PKL/{}'.format(bucket, fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = load_files('full_descriptions.pkl')\n",
    "features = load_files('full_features.pkl')\n",
    "train_list_full = load_files('train_list_full.pkl')\n",
    "val_list_full = load_files('val_list_full.pkl')\n",
    "test_list = load_files('test_list.pkl')\n",
    "test_list_art = load_files('test_list_art.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "processor = sequence_generator(descriptions, features)\n",
    "\n",
    "train_X1, train_X2, train_Y = processor.train_generator(train_list_full)\n",
    "val_X1, val_X2, val_Y = processor.validation_generator(val_list_full)\n",
    "\n",
    "# get params\n",
    "tokenizer = processor.get_tokenizer()\n",
    "max_length = processor.get_max_length()\n",
    "num_vocab = processor.get_num_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Mode\n",
    "Run it on the local mode to check the performance first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point = 'SCRIPT/nlg_model_tf_1.py', \n",
    "                         role = role, \n",
    "                         train_instance_count = 1, \n",
    "                         train_instance_type = 'local', \n",
    "                         py_version = 'py37', \n",
    "                         framework_version = '2.2',\n",
    "                         script_mode = True, \n",
    "                         hyperparameters = {'epochs': epochs, \n",
    "                                           'batch_size': batch_size}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(pkl_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit(train_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point = 'SCRIPT/nlg_model_tf_1.py', \n",
    "                         role = role, \n",
    "                         train_instance_count = 1, \n",
    "                         train_instance_type = 'ml.p2.xlarge',\n",
    "                         py_version = 'py37', \n",
    "                         framework_version = '1.15.2',\n",
    "                         script_mode = True, \n",
    "                         hyperparameters = {'epochs': epochs, \n",
    "                                           'batch-size': batch_size}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit(train_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_endpoint = 'nlg-model-tf-'+time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "# GPU\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count = 1, \n",
    "                                  instance_type = 'ml.p2.xlarge', \n",
    "                                  endpoint_name = tf_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete End-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name = tf_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
