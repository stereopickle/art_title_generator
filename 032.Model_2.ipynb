{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2\n",
    "This notebook is a deeper iteration of similar architecture from model 1 set up for the purpose of running on AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import tensorflow as tf\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'art-descriptor-paintings'\n",
    "pkl_dir = f's3://{bucket}/PKL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_path        = os.path.join(pkl_dir, 'condensed_descriptions.pkl')\n",
    "feat_path        = os.path.join(pkl_dir, 'full_features.pkl')\n",
    "train_list_path  = os.path.join(pkl_dir, 'train_list_full.pkl')\n",
    "val_list_path    = os.path.join(pkl_dir, 'val_list_full.pkl')\n",
    "test_list_path   = os.path.join(pkl_dir, 'test_list.pkl')\n",
    "test_art_path    = os.path.join(pkl_dir, 'test_list_art.pkl')\n",
    "\n",
    "with open(train_list_path, 'rb') as fp:\n",
    "    train_list_full = pickle.load(fp)\n",
    "\n",
    "with open(val_list_path, 'rb') as fp:\n",
    "    val_list_full = pickle.load(fp)\n",
    "\n",
    "with open(test_list_path, 'rb') as fp:\n",
    "    test_list = pickle.load(fp)\n",
    "\n",
    "with open(test_art_path, 'rb') as fp:\n",
    "    test_list_art = pickle.load(fp)\n",
    "\n",
    "with open(desc_path, 'rb') as fp:\n",
    "    descriptions = pickle.load(fp)\n",
    "\n",
    "with open(feat_path, 'rb') as fp:\n",
    "    features = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SCRIPT.sequence_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = sequence_generator(descriptions, features)\n",
    "\n",
    "train_X1, train_X2, train_Y = processor.train_generator(train_list_full)\n",
    "val_X1, val_X2, val_Y = processor.validation_generator(val_list_full)\n",
    "\n",
    "# get params\n",
    "tokenizer = processor.get_tokenizer()\n",
    "max_length = processor.get_max_length()\n",
    "num_vocab = processor.get_num_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point = 'SCRIPT/nlg_model_tf_1.py', \n",
    "                         role = role, \n",
    "                         train_instance_count = 1, \n",
    "                         train_instance_type = 'local', \n",
    "                         py_version = 'py37', \n",
    "                         framework_version = '1.15.2',\n",
    "                         script_mode = True, \n",
    "                         hyperparameters = {'epochs': epochs, \n",
    "                                           'batch-size': batch_size}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'pkl': pkl_dir})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point = 'SCRIPT/nlg_model_tf_1.py', \n",
    "                         role = role, \n",
    "                         train_instance_count = 1, \n",
    "                         train_instance_type = '',\n",
    "                         py_version = 'py37', \n",
    "                         framework_version = '1.15.2',\n",
    "                         script_mode = True, \n",
    "                         hyperparameters = {'epochs': epochs, \n",
    "                                           'batch-size': batch_size}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'pkl': pkl_dir})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tf_endpoint = 'nlg-model-tf-'+time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "# GPU\n",
    "#tf_predictor = tf_estimator.deploy(initial_instance_count = 1, \n",
    "#                                   instance_type = 'ml.p2.xlarge')\n",
    "\n",
    "\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count = 1, \n",
    "                                  instance_type = 'ml.c5.large', \n",
    "                                  accelerator_type = 'ml.eia1.medium', \n",
    "                                  endpoint_name = tf_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SCRIPT.evaluation_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing on photo data\n",
    "pred_list, score = get_bleu(test_list, \n",
    "                            features, \n",
    "                            tokenizer, \n",
    "                            max_length, \n",
    "                            tf_predictor, \n",
    "                            descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on photo data\n",
    "artpred_list, artscore = get_bleu(test_list_art, \n",
    "                            features, \n",
    "                            tokenizer, \n",
    "                            max_length, \n",
    "                            tf_predictor, \n",
    "                            descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_descriptor = descriptor(features, tokenizer, processor, model)\n",
    "#flicker_descriptor.test_random_image(img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#art_descriptor = descriptor(features, tokenizer, processor, model,img_dir = art_dir)\n",
    "#art_descriptor.test_one_image(img_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete End-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name = tf_endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
