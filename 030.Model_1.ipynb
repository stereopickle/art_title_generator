{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Object Captioning\n",
    "\n",
    "Here I will run a model to generate captions for objects. This will be done in a few step.  \n",
    "(the main structure of steps and dataset from https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)\n",
    "\n",
    "1. Extract feature from pretrained network for object recognition.  \n",
    "2. Using the flicker dataset with labels, train CNN/RNN model \n",
    "3. Art Captioning: for this I'll try couple different method\n",
    "    - test artworks on a model trained for object captioning\n",
    "    - include art with description to the CNN/RNN \n",
    "    - include artworks to the CNN\n",
    "    - include less concrete object images (e.g. cloud) to the CNN\n",
    "    - train more abstract labeling in RNN\n",
    "    \n",
    "Additionally, some evaluation steps to take a look.\n",
    "1. Check what type of objects are classified better or worse\n",
    "2. Try semantic projection (abstract - concrete) to score the level of abstraction of each word and run statistical testing on how the model performance differs per the level of abstraction\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import NASNetLarge \n",
    "from tensorflow.keras.applications.nasnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flicker_img_dir = 'IMAGES/Flicker/Flicker8k_Dataset'\n",
    "flicker_text_dir = 'IMAGES/Flicker/labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "First, extracting features from Flicker dataset using NASNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(dir_, network):\n",
    "    ''' \n",
    "    iterate through files in dir_ \n",
    "    and get features running on network\n",
    "    return a dictionary with image id as a key\n",
    "    '''\n",
    "    model = network(include_top = False)\n",
    "    fnames = [x for x in os.listdir(dir_) if x.endswith('.jpg')]\n",
    "    result = {}\n",
    "    i = 1\n",
    "    n = len(fnames)\n",
    "    \n",
    "    for fn in fnames:\n",
    "        img = load_img(f'{dir_}/{fn}', target_size = (model.input.shape[1], model.input.shape[2]))\n",
    "        img = np.expand_dims(img, 0)\n",
    "        img = preprocess_input(img)\n",
    "        feature = model.predict(img)\n",
    "        ind = fn[:-4]\n",
    "        result[ind] = feature\n",
    "        print(f'{i}/{n} feature extraction completed')\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = feature_extractor(flicker_img_dir, NASNetLarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the extracted features\n",
    "with open('PKL/features.pkl', 'wb') as fp:\n",
    "    pickle.dump(features, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('PKL/features.pkl', 'rb') as fp:\n",
    "    features = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Description\n",
    "Now I will clean up the descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the description file\n",
    "with open(f'{flicker_text_dir}/Flickr8k.token.txt', 'r') as fn:\n",
    "    text = fn.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a dictionary with the image id as a key and the descriptions associated with that id as value.  \n",
    "While I'm at it I'll also remove punctuations, make them lowercase, remove a single letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only image id and description\n",
    "import re\n",
    "pattern = '([0-9a-z_]*)\\.jpg.*\\\\t(.*)\\\\n'\n",
    "p = re.compile(pattern)\n",
    "descriptions_pairs = [p.findall(x)[0] for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "descriptions = {}\n",
    "\n",
    "table_ = str.maketrans('', '', string.punctuation+string.digits)\n",
    "\n",
    "for ind, text in descriptions_pairs:\n",
    "    text = text.lower()\n",
    "    text = str.translate(text, table_)\n",
    "    text = [x for x in text.split() if len(x) > 1] # remove trailing alphabet\n",
    "    text = 'seqini ' + ' '.join(text) + ' seqfin' # add tokens\n",
    "    if ind in descriptions:\n",
    "        descriptions[ind].append(text)\n",
    "    else:\n",
    "        descriptions[ind] = [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the description keys\n",
    "with open('PKL/descriptions.pkl', 'wb') as fp:\n",
    "    pickle.dump(descriptions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('PKL/descriptions.pkl', 'rb') as fp:\n",
    "    descriptions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8767"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocabulary_counts = len(set(' '.join(np.concatenate(list(descriptions.values()))).split()))\n",
    "total_vocabulary_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Val Split\n",
    "Now I'll split the list of flicker images into train/test sets then create a function that filters the dictionary of descriptions by a given list of image id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_list, test_list = train_test_split(list(descriptions.keys()), test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Inputs\n",
    "Now that we have our dictionaries of features and descriptions. We need to create input and output series.  \n",
    "We need two separate inputs: image features, description as sequences. The output is the next word in the sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(dict_):\n",
    "    ''' Helper to return a list of keys '''\n",
    "    return list(dict_.keys())\n",
    "\n",
    "def get_vals(dict_):\n",
    "    ''' Helper to return a list of values '''\n",
    "    return list(dict_.values())\n",
    "\n",
    "def breakdown_sequence(list_):\n",
    "    ''' Helper to return a list of breakdown sequences and the output '''\n",
    "    x, y = [], []\n",
    "    for i in range(1, len(list_)):\n",
    "        x.append(list_[:i])\n",
    "        y.append(list_[i])\n",
    "    return x, y\n",
    "\n",
    "def sequence_process(select_dictionary):\n",
    "    ''' Helper to process breakdown on all select dictionary '''\n",
    "    X1, X2, Y = [], [], []\n",
    "\n",
    "    for ind, texts in select_dictionary.items():\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        for seq in sequences:\n",
    "            x, y = breakdown_sequence(seq)\n",
    "            X1.extend(np.repeat(ind, len(y)))\n",
    "            X2.extend(x)\n",
    "            Y.extend(y)\n",
    "\n",
    "    return X1, X2, Y\n",
    "    \n",
    "    \n",
    "class sequence_generator:\n",
    "    def __init__(self, dictionary):\n",
    "        ''' INPUT: a dictionary of descriptions '''\n",
    "        self.dictionary = dictionary\n",
    "        self.img_index = get_keys(self.dictionary)\n",
    "        self.texts = get_vals(self.dictionary)\n",
    "\n",
    "    def get_text(self, img_ind):\n",
    "        ''' RETURN: a list of description given an id '''\n",
    "        return self.dictionary[img_ind]\n",
    "\n",
    "    def update_selection(self, list_):\n",
    "        ''' Helper to update selector, dictionary, img, texts '''\n",
    "        self.selector = list_\n",
    "        self.select_dictionary = {k: v for k, v in self.dictionary.items() if k in list_}\n",
    "        self.select_img_inds = get_keys(self.select_dictionary)\n",
    "        self.select_texts = get_vals(self.select_dictionary)\n",
    "\n",
    "    def train_generator(self, train_list):\n",
    "        '''\n",
    "        INPUT a list of training ids, \n",
    "        RETURN image inputs, text inputs, and outputs\n",
    "        ASSIGN max_length and vocab size\n",
    "        '''\n",
    "        self.update_selection(train_list)\n",
    "\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(np.concatenate(self.select_texts))\n",
    "        self.num_vocab = len(self.tokenizer.word_index)+1\n",
    "\n",
    "        X1, X2, Y = sequence_process(self.select_dictionary)\n",
    "\n",
    "        X2 = pad_sequences(X2)\n",
    "        self.max_length = X2.shape[1]\n",
    "    \n",
    "        Y = to_categorical(Y, self.num_vocab)\n",
    "        return X1, X2, Y\n",
    "\n",
    "    def validation_generator(self, val_list):\n",
    "        '''\n",
    "        INPUT a list of validation ids, \n",
    "        RETURN image inputs, text inputs and outputs\n",
    "        '''\n",
    "        self.update_selection(val_list)\n",
    "\n",
    "        X1, X2, Y = sequence_process(self.select_dictionary)\n",
    "        X2 = pad_sequences(X2, maxlen = self.max_length)\n",
    "        Y = to_categorical(Y, self.num_vocab)\n",
    "        return X1, X2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = sequence_generator(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1, train_X2, train_Y = processor.train_generator(train_list[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X1, val_X2, val_Y = processor.train_generator(test_list[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
